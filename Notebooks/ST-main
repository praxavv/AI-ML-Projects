{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj-9VxLQO4S4"
      },
      "source": [
        "> Project Title: **Sustainability-Tracker**\n",
        "\n",
        "*Interactive Weather Forecasting with BigQuery AI*\n",
        "\n",
        "**Problem Statement:**\n",
        "Accurate short-term temperature prediction is essential for agriculture, energy planning, and disaster preparedness. Traditional weather dashboards either overwhelm users with raw data or hide the logic behind black-box forecasts. This project tackles the challenge of making weather prediction both accurate and transparent by combining historical GSOD data, machine learning models, and interactive visualizations into a single, user-friendly platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM4V9KZlr7qm"
      },
      "outputs": [],
      "source": [
        "# --- Setup & Auth ---\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate & init BigQuery\n",
        "auth.authenticate_user()\n",
        "PROJECT_ID = \"sustainability-tracker-469906\"\n",
        "DATASET_ID = \"weather_models\"\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(\"✅ GCP authentication complete & BigQuery client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jd8l-msQdtx"
      },
      "source": [
        "Data Exploration:\n",
        "We leveraged **NOAA’s GSOD** dataset (2018–2024) hosted on BigQuery as the foundation for our analysis. The exploration phase began by examining dataset scale—counting the total rows and inspecting the last ten records—to confirm consistency and completeness. We then extracted metadata, including overall shape, column count, and variable names, to build a clear understanding of the dataset’s structure before moving into preprocessing and modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-5Xc9EFv0XBH"
      },
      "outputs": [],
      "source": [
        "# Step 2: LOADING DATASET.....\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Assume client is already initialized\n",
        "\n",
        "# SQL query: fetch last 10 rows of the dataset + total row count\n",
        "query = \"\"\"\n",
        "WITH data AS (\n",
        "  SELECT *\n",
        "  FROM `bigquery-public-data.noaa_gsod.gsod*`\n",
        "  WHERE _TABLE_SUFFIX BETWEEN '2018' AND '2024'\n",
        "),\n",
        "row_count AS (\n",
        "  SELECT COUNT(*) AS total_rows FROM data\n",
        "),\n",
        "last_ten AS (\n",
        "  SELECT *\n",
        "  FROM data\n",
        "  ORDER BY date DESC\n",
        "  LIMIT 10\n",
        ")\n",
        "SELECT l.*, r.total_rows\n",
        "FROM last_ten l\n",
        "CROSS JOIN row_count r\n",
        "\"\"\"\n",
        "\n",
        "# Run query and load into DataFrame\n",
        "dataset = client.query(query).to_dataframe()\n",
        "\n",
        "# Extract dataset metadata\n",
        "total_rows = int(dataset['total_rows'].iloc[0])\n",
        "shape = (total_rows, dataset.shape[1] - 1)  # exclude helper column\n",
        "columns = list(dataset.drop(columns=['total_rows']).columns)\n",
        "\n",
        "# Print metadata in plain text\n",
        "print(\"\\nDataset Metadata:\")\n",
        "print(f\"Total Rows: {total_rows}\")\n",
        "print(f\"Shape: {shape}\")\n",
        "print(f\"Column Count: {shape[1]}\")\n",
        "print(\"Columns:\", columns)\n",
        "\n",
        "print(\"\\nLast 10 rows:\")\n",
        "\n",
        "# Show last 10 rows (transposed for readability)\n",
        "display(dataset.drop(columns=[\"total_rows\"]).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4vOEgOSVbtbW"
      },
      "outputs": [],
      "source": [
        "# After Pre-processing\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1️⃣ Let'see the last 10 rows\n",
        "query_last_10 = f\"\"\"\n",
        "SELECT *\n",
        "FROM (\n",
        "  SELECT *\n",
        "  FROM `{PROJECT_ID}.{DATASET_ID}.gsod_daily_aggregated`\n",
        "  ORDER BY date DESC\n",
        "  LIMIT 10\n",
        ")\n",
        "ORDER BY date ASC, station_name ASC\n",
        "\"\"\"\n",
        "df_last_10 = client.query(query_last_10).to_dataframe()\n",
        "display(df_last_10.T)\n",
        "\n",
        "# 2️⃣ Get the total number of rows and columns in the full table\n",
        "query_shape = f\"\"\"\n",
        "SELECT COUNT(*) AS total_rows\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.gsod_daily_aggregated`\n",
        "\"\"\"\n",
        "total_rows = client.query(query_shape).to_dataframe()['total_rows'][0]\n",
        "total_cols = len(df_last_10.columns)  # or you can fetch schema to get actual column count\n",
        "print(f\"Full dataset shape: ({total_rows}, {total_cols})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfL_l92Qu_z"
      },
      "source": [
        "Step 3: Preprocessed Daily Dataset\n",
        "The gsod_daily_aggregated table holds cleaned and standardized daily weather observations, ready for analysis and modeling.\n",
        "\n",
        "Preprocessing highlights:\n",
        "\n",
        "a. Converted units for consistency (°F → °C, mph → m/s)\n",
        "\n",
        "b. Removed invalid or missing values\n",
        "\n",
        "c. Enriched records by joining station metadata (name, country, latitude, longitude)\n",
        "\n",
        "To validate these transformations, we inspected the last 10 rows and confirmed the overall dataset shape, ensuring the preprocessing pipeline produced a reliable foundation for modeling.\n",
        "\n",
        "> Note on Precision:\n",
        "\n",
        "\n",
        "Predictions are reported with high decimal precision to preserve accuracy. While this may produce outputs with many decimal places, the detail ensures results remain statistically robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rUpBLjh5HqhN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Fetch yearly aggregated stats from daily data\n",
        "query_yearly = f\"\"\"\n",
        "WITH daily_stats AS (\n",
        "  SELECT\n",
        "    EXTRACT(YEAR FROM date) AS year,\n",
        "    AVG(temp_c) AS avg_temp,\n",
        "    MIN(temp_c) AS min_temp,\n",
        "    MAX(temp_c) AS max_temp,\n",
        "    AVG(precipitation_mm) AS avg_precip,\n",
        "    SUM(precipitation_mm) AS total_precip,\n",
        "    AVG(wind_speed_m_s) AS avg_wind\n",
        "  FROM `{PROJECT_ID}.{DATASET_ID}.gsod_daily_aggregated`\n",
        "  GROUP BY year\n",
        ")\n",
        "SELECT * FROM daily_stats\n",
        "ORDER BY year\n",
        "\"\"\"\n",
        "df_yearly = client.query(query_yearly).to_dataframe()\n",
        "\n",
        "# Set scientific style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.figsize': (15,10), 'font.size': 12})\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "\n",
        "# 1. Temperature trends with min-max band\n",
        "axs[0,0].plot(df_yearly.year, df_yearly.avg_temp, color='tomato', label='Mean Temp')\n",
        "axs[0,0].fill_between(df_yearly.year, df_yearly.min_temp, df_yearly.max_temp, color='tomato', alpha=0.2, label='Min-Max')\n",
        "axs[0,0].set_title(\"Annual Temperature Trends (°C)\")\n",
        "axs[0,0].set_xlabel(\"Year\"); axs[0,0].set_ylabel(\"Temperature (°C)\")\n",
        "axs[0,0].legend()\n",
        "\n",
        "# 2. Precipitation trends\n",
        "axs[0,1].bar(df_yearly.year, df_yearly.total_precip, color='royalblue', alpha=0.7)\n",
        "axs[0,1].plot(df_yearly.year, df_yearly.avg_precip, color='navy', marker='o', label='Avg Precip')\n",
        "axs[0,1].set_title(\"Annual Precipitation (mm)\")\n",
        "axs[0,1].set_xlabel(\"Year\"); axs[0,1].set_ylabel(\"Precipitation (mm)\")\n",
        "axs[0,1].legend()\n",
        "\n",
        "# 3. Wind trends\n",
        "axs[1,0].plot(df_yearly.year, df_yearly.avg_wind, color='seagreen', marker='s')\n",
        "axs[1,0].set_title(\"Average Annual Wind Speed (m/s)\")\n",
        "axs[1,0].set_xlabel(\"Year\"); axs[1,0].set_ylabel(\"Wind Speed (m/s)\")\n",
        "\n",
        "# 4. Correlation Heatmap\n",
        "corr = df_yearly[['avg_temp','avg_precip','avg_wind']].corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', ax=axs[1,1], square=True)\n",
        "axs[1,1].set_title(\"Climate Metrics Correlation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoYtAG3gdlsb"
      },
      "source": [
        "**Yearly Climate Statistics**\n",
        "\n",
        "Daily data was aggregated to compute annual statistics across temperature, precipitation, and wind.\n",
        "\n",
        "> Visualizations include:\n",
        "\n",
        "**Temperature**: annual mean with min–max range\n",
        "\n",
        "**Precipitation**: yearly totals and average trends\n",
        "\n",
        "**Wind**: average annual speed\n",
        "\n",
        "**Correlation**: relationships between key climate metrics\n",
        "\n",
        "Together, these plots offer a high-level view of long-term climate patterns.\n",
        "\n",
        "*Note: The visualizations are based on NOAA datasets and are intended to provide insights into climate trends. Actual local conditions may differ and should be interpreted with context.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrWggMRQ7rP"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# ----- assume PROJECT_ID, DATASET_ID, client already defined -----\n",
        "\n",
        "# Fetch data\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    station_id,\n",
        "    station_name,\n",
        "    country_code,\n",
        "    latitude,\n",
        "    longitude,\n",
        "    AVG(temp_c) AS temp_c,\n",
        "    AVG(dewp_c) AS dewp_c,\n",
        "    AVG(wind_speed_m_s) AS wind_speed_m_s,\n",
        "    AVG(visibility_km) AS visibility_km,\n",
        "    AVG(precipitation_mm) AS precipitation_mm,\n",
        "    AVG(pressure_hpa) AS pressure_hpa\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.gsod_daily_aggregated`\n",
        "GROUP BY station_id, station_name, country_code, latitude, longitude\n",
        "\"\"\"\n",
        "df = client.query(query).to_dataframe().round(4)\n",
        "\n",
        "# Convert lat/lon to spherical coordinates\n",
        "lat_rad = np.radians(df['latitude'])\n",
        "lon_rad = np.radians(df['longitude'])\n",
        "R = 1  # radius of globe\n",
        "x = R * np.cos(lat_rad) * np.cos(lon_rad)\n",
        "y = R * np.cos(lat_rad) * np.sin(lon_rad)\n",
        "z = R * np.sin(lat_rad)\n",
        "\n",
        "# Use actual temperature values for color (no normalization)\n",
        "temps = df['temp_c']\n",
        "\n",
        "# Hover text\n",
        "hover_text = [\n",
        "    f\"<b>{row['station_name']} ({row['country_code']})</b><br><br>\"\n",
        "    f\"Avg Temp: {row['temp_c']} °C<br>\"\n",
        "    f\"Avg Dewp: {row['dewp_c']} °C<br>\"\n",
        "    f\"Avg Wind: {row['wind_speed_m_s']} m/s<br>\"\n",
        "    f\"Avg Visibility: {row['visibility_km']} km<br>\"\n",
        "    f\"Avg Precipitation: {row['precipitation_mm']} mm<br>\"\n",
        "    f\"Avg Pressure: {row['pressure_hpa']} hPa\"\n",
        "    for idx, row in df.iterrows()\n",
        "]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Solid black globe\n",
        "phi, theta = np.mgrid[0:np.pi:50j, 0:2*np.pi:100j]\n",
        "xs = R * np.sin(phi) * np.cos(theta)\n",
        "ys = R * np.sin(phi) * np.sin(theta)\n",
        "zs = R * np.cos(phi)\n",
        "\n",
        "fig.add_trace(go.Surface(\n",
        "    x=xs, y=ys, z=zs,\n",
        "    colorscale=[[0, \"black\"], [1, \"black\"]],\n",
        "    showscale=False,\n",
        "    hoverinfo='skip',\n",
        "    opacity=1.0\n",
        "))\n",
        "\n",
        "# Colorscale: -40°C -> blue, 0°C -> white, +50°C -> neon orange\n",
        "BLUE        = '#0066FF'   # strong blue for cold\n",
        "WHITE       = '#FFFFFF'   # neutral white at freezing point\n",
        "NEON_ORANGE = '#FF6A00'   # neon orange for heat\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=x, y=y, z=z,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=2,\n",
        "        sizemode=\"diameter\",\n",
        "        color=temps,\n",
        "        colorscale=[\n",
        "            [0.0, BLUE],       # maps to -40°C\n",
        "            [0.44, WHITE],     # maps to 0°C (relative to -40 → +50 range)\n",
        "            [1.0, NEON_ORANGE] # maps to +50°C\n",
        "        ],\n",
        "        cmin=-40,\n",
        "        cmax=50,\n",
        "        colorbar=dict(title=\"Avg Temp (°C)\", thickness=15, len=0.5, x=1.05, y=0.8), # adjust position to sit above the colorbar\n",
        "        opacity=1.0,\n",
        "        line=dict(width=0.2, color='black')\n",
        "    ),\n",
        "    hoverinfo='skip',\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "\n",
        "# Invisible larger markers that actually carry the hover (stable hitbox)\n",
        "# Adjust HOVER_SIZE to change how big the hover-sensitive area is\n",
        "HOVER_SIZE = 10\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=x, y=y, z=z,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=HOVER_SIZE,\n",
        "        sizemode=\"diameter\",\n",
        "        color='rgba(0,0,0,0)',  # fully transparent\n",
        "        opacity=0.0,            # invisible, but still hoverable\n",
        "    ),\n",
        "    hovertext=hover_text,\n",
        "    hoverinfo='text',\n",
        "    hoverlabel=dict(\n",
        "        bgcolor='black',\n",
        "        font_size=12,\n",
        "        font_color='#39FF14',\n",
        "        bordercolor='#39FF14',\n",
        "        namelength=0\n",
        "    ),\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "# Layout for solid black and no axes\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(visible=False),\n",
        "        yaxis=dict(visible=False),\n",
        "        zaxis=dict(visible=False),\n",
        "        bgcolor='black',\n",
        "        aspectmode='data',\n",
        "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.0))\n",
        "    ),\n",
        "    paper_bgcolor='black',\n",
        "    plot_bgcolor='black',\n",
        "    margin=dict(l=0, r=0, t=50, b=0),\n",
        "    title=\"🌐 Weather Stations Globe\",\n",
        "    font=dict(size=10, color='white'),\n",
        "    hovermode='closest',   # keep closest hover behaviour\n",
        "    # hoverdistance doesn't always affect 3D the same as 2D, but keep it:\n",
        "    hoverdistance=2\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2bB4XfNgK23"
      },
      "source": [
        "This prototype pulls aggregated weather data from BigQuery and plots it on a 3D globe using Plotly. Each station is mapped by latitude and longitude, with temperature encoded in a custom color scale (blue for cold, white at freezing, neon orange for heat). Hover tooltips show station details and climate averages, while invisible hitboxes keep interaction smooth. The result is a clean, interactive globe for exploring global weather patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW8p6hp-_wPV"
      },
      "outputs": [],
      "source": [
        "import requests, joblib, pandas as pd\n",
        "\n",
        "# 1. Download the model file from GitHub raw\n",
        "url = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "r = requests.get(url)\n",
        "with open(\"temp_c_boosted.pkl\", \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "# 2. Load the model\n",
        "model = joblib.load(\"temp_c_boosted.pkl\")\n",
        "\n",
        "# 3. Example prediction\n",
        "example_input = pd.DataFrame([{\n",
        "    'lat': 19.0785, 'lon': 72.8782, 'month': 9, 'day': 3,\n",
        "    'dewp_c': 25, 'wind_speed_m_s': 2, 'visibility_km': 10,\n",
        "    'precipitation_mm': 0, 'pressure_hpa': 1012\n",
        "}])\n",
        "\n",
        "pred = model.predict(example_input)\n",
        "print(\"Example prediction:\", round(pred[0], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdmALeuwqrdz"
      },
      "source": [
        "“We queried historical weather data from BigQuery, selected relevant features, and trained an XGBoost regressor to predict daily temperature. The model was saved as temp_c_boosted.pkl for inference and demonstration.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Gi7iLj8Sjmo"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
        "\n",
        "# ----------------- Fetch model from GitHub -----------------\n",
        "url = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "local_model_path = \"temp_c_boosted.pkl\"\n",
        "\n",
        "# Always overwrite with fresh version\n",
        "if os.path.exists(local_model_path):\n",
        "    os.remove(local_model_path)\n",
        "\n",
        "urllib.request.urlretrieve(url, local_model_path)\n",
        "model = joblib.load(local_model_path)\n",
        "\n",
        "# ----------------- Load validation set -----------------\n",
        "client = bigquery.Client()\n",
        "val_table = \"sustainability-tracker-469906.weather_models.gsod_validation_features\"\n",
        "df_val = client.query(f\"SELECT * FROM `{val_table}`\").to_dataframe()\n",
        "\n",
        "# ----------------- Features & target -----------------\n",
        "features = [\n",
        "    'lat', 'lon', 'month', 'day', 'dewp_c', 'wind_speed_m_s',\n",
        "    'visibility_km', 'precipitation_mm', 'pressure_hpa'\n",
        "]\n",
        "target = 'target'\n",
        "\n",
        "# ----------------- Evaluate per val_window -----------------\n",
        "results = []\n",
        "for window, group in df_val.groupby(\"val_window\"):\n",
        "    X_val = group[features]\n",
        "    y_val = group[target]\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    r2 = r2_score(y_val, preds)\n",
        "    ev = explained_variance_score(y_val, preds)\n",
        "\n",
        "    results.append({\n",
        "        \"val_window\": window,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2,\n",
        "        \"Explained_Variance\": ev\n",
        "    })\n",
        "\n",
        "# ----------------- Convert to DataFrame -----------------\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 Per-window performance:\")\n",
        "print(df_results)\n",
        "\n",
        "# ----------------- Calculate overall averages -----------------\n",
        "overall_metrics = df_results[[\"RMSE\", \"MAE\", \"R2\", \"Explained_Variance\"]].mean()\n",
        "print(\"\\n🔮 Overall validation performance:\")\n",
        "print(overall_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRU2BIAbncdH"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- `val_1` shows the strongest performance with the lowest errors and highest R².  \n",
        "- `val_2` and `val_3` exhibit slightly higher errors, indicating some variance across different windows.  \n",
        "- `val_4` has the lowest R² (0.879), suggesting the model struggles a bit in this window—possibly due to outlier conditions or underrepresented patterns.  \n",
        "- `val_5` shows a rebound in performance, demonstrating the model's general stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "npHVjnl1dqtE"
      },
      "outputs": [],
      "source": [
        "# ----------------- Config -----------------\n",
        "import urllib.request\n",
        "import joblib\n",
        "\n",
        "MODEL_URL = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "MODEL_PATH = \"temp_c_boosted.pkl\"\n",
        "INFERENCE_TABLE = \"sustainability-tracker-469906.weather_models.gsod_inference\"\n",
        "TRAIN_TABLE = \"sustainability-tracker-469906.weather_models.gsod_train_features\"\n",
        "AGG_TABLE = \"sustainability-tracker-469906.weather_models.gsod_daily_aggregated\"\n",
        "\n",
        "# ----------------- Load model -----------------\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    os.remove(MODEL_PATH)\n",
        "urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
        "model = joblib.load(MODEL_PATH)\n",
        "print(f\"✅ Model loaded from GitHub: {MODEL_PATH}\")\n",
        "\n",
        "# ----------------- BigQuery client -----------------\n",
        "client = bigquery.Client()\n",
        "\n",
        "# ----------------- Load inference data (Sep–Dec) -----------------\n",
        "df_inf = client.query(f\"SELECT * FROM `{INFERENCE_TABLE}`\").to_dataframe()\n",
        "df_inf = df_inf[df_inf[\"month\"].between(9,12)].copy()\n",
        "df_inf.rename(columns={\"latitude\":\"lat\",\"longitude\":\"lon\"}, inplace=True)\n",
        "print(f\"✅ Inference data (Sep–Dec) loaded: {len(df_inf)} rows\")\n",
        "\n",
        "# ----------------- Historical averages -----------------\n",
        "query_hist = f\"\"\"\n",
        "SELECT lat, lon,\n",
        "       AVG(dewp_c) AS dewp_c,\n",
        "       AVG(wind_speed_m_s) AS wind_speed_m_s,\n",
        "       AVG(visibility_km) AS visibility_km,\n",
        "       AVG(precipitation_mm) AS precipitation_mm,\n",
        "       AVG(pressure_hpa) AS pressure_hpa\n",
        "FROM `{TRAIN_TABLE}`\n",
        "GROUP BY lat, lon\n",
        "\"\"\"\n",
        "df_hist = client.query(query_hist).to_dataframe()\n",
        "df_inf = df_inf.merge(df_hist, on=[\"lat\",\"lon\"], how=\"left\")\n",
        "print(f\"✅ Historical averages merged: {len(df_inf)} rows\")\n",
        "\n",
        "# ----------------- Make predictions -----------------\n",
        "features = ['lat','lon','month','day','dewp_c','wind_speed_m_s','visibility_km','precipitation_mm','pressure_hpa']\n",
        "df_inf[\"prediction\"] = model.predict(df_inf[features])\n",
        "df_inf[\"date\"] = pd.to_datetime(dict(year=2024, month=df_inf[\"month\"], day=df_inf[\"day\"]), errors=\"coerce\").dt.date\n",
        "\n",
        "# ----------------- Load actuals -----------------\n",
        "query_actuals = f\"\"\"\n",
        "SELECT station_id, date, temp_c\n",
        "FROM `{AGG_TABLE}`\n",
        "WHERE date BETWEEN '2024-09-01' AND '2024-12-31'\n",
        "\"\"\"\n",
        "df_actuals = client.query(query_actuals).to_dataframe()\n",
        "df_actuals[\"date\"] = pd.to_datetime(df_actuals[\"date\"]).dt.date\n",
        "print(f\"✅ Actual temperatures loaded: {len(df_actuals)} rows\")\n",
        "\n",
        "# ----------------- Merge predictions with actuals -----------------\n",
        "df_results = df_inf.merge(df_actuals, on=[\"station_id\",\"date\"], how=\"left\").rename(columns={\"temp_c\":\"actual_temp\"})\n",
        "\n",
        "# ----------------- Compute errors -----------------\n",
        "if df_results[\"actual_temp\"].notna().any():\n",
        "    df_results[\"error\"] = df_results[\"prediction\"] - df_results[\"actual_temp\"]\n",
        "    df_results[\"abs_error\"] = df_results[\"error\"].abs()\n",
        "\n",
        "    # Standard MAE & RMSE\n",
        "    mae = df_results['abs_error'].mean()\n",
        "    rmse = np.sqrt((df_results['error']**2).mean())\n",
        "\n",
        "    # ----------------- Robust MAE & RMSE (exclude top 1% outliers) -----------------\n",
        "    threshold = np.percentile(df_results['abs_error'], 99)\n",
        "    df_robust = df_results[df_results['abs_error'] <= threshold]\n",
        "    robust_mae = df_robust['abs_error'].mean()\n",
        "    robust_rmse = np.sqrt((df_robust['error']**2).mean())\n",
        "\n",
        "    print(\"\\n📊 Evaluation Metrics (Sep–Dec 2024):\")\n",
        "    print(f\"   MAE  = {mae:.3f}\")\n",
        "    print(f\"   RMSE = {rmse:.3f}\")\n",
        "    print(f\"   Robust MAE  = {robust_mae:.3f} (excluding top 1% errors)\")\n",
        "    print(f\"   Robust RMSE = {robust_rmse:.3f} (excluding top 1% errors)\")\n",
        "\n",
        "# ----------------- Major capitals -----------------\n",
        "capitals = {\n",
        "    \"WASHINGTON D.C.\": {\"lat\": 38.8951, \"lon\": -77.0364, \"country_code\":\"US\"},\n",
        "    \"BEIJING\": {\"lat\": 39.9042, \"lon\": 116.4074, \"country_code\":\"CN\"},\n",
        "    \"MOSCOW\": {\"lat\": 55.7558, \"lon\": 37.6173, \"country_code\":\"RU\"},\n",
        "    \"NEW DELHI\": {\"lat\": 28.6139, \"lon\": 77.2090, \"country_code\":\"IN\"},\n",
        "    \"LONDON\": {\"lat\": 51.5074, \"lon\": -0.1278, \"country_code\":\"GB\"}\n",
        "}\n",
        "\n",
        "# ----------------- Assign country rank and sort all stations -----------------\n",
        "country_order = [\"US\",\"CN\",\"RU\",\"IN\",\"GB\"]\n",
        "df_results[\"country_rank\"] = df_results[\"country_code\"].apply(lambda x: country_order.index(x) if x in country_order else 999)\n",
        "df_results.sort_values([\"country_rank\",\"date\"], ascending=[True, False], inplace=True)\n",
        "\n",
        "# ----------------- Find nearest station for each capital -----------------\n",
        "nearest_rows = []\n",
        "for name, info in capitals.items():\n",
        "    df_results[\"dist_to_capital\"] = ((df_results[\"lat\"] - info[\"lat\"])**2 + (df_results[\"lon\"] - info[\"lon\"])**2)**0.5\n",
        "    nearest = df_results.loc[df_results[\"dist_to_capital\"].idxmin()].copy()\n",
        "    nearest[\"capital_name\"] = name\n",
        "    nearest_rows.append(nearest)\n",
        "\n",
        "df_capitals = pd.DataFrame(nearest_rows)\n",
        "\n",
        "# ----------------- Display final table -----------------\n",
        "display(df_capitals[[\"capital_name\",\"station_name\",\"lat\",\"lon\",\"date\",\"prediction\",\"actual_temp\"]])\n",
        "\n",
        "# ----------------- Prepare final CSV -----------------\n",
        "df_final_csv = df_results[[\"station_name\",\"country_code\",\"date\",\"prediction\",\"actual_temp\"]].copy()\n",
        "\n",
        "# ----------------- Save CSV -----------------\n",
        "csv_path = \"test_predictions_clean.csv\"\n",
        "df_final_csv.to_csv(csv_path, index=False)\n",
        "print(f\"✅ CSV saved: {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhh5VV8QoFy_"
      },
      "source": [
        "> **Inference Summary: Sep–Dec 2024**\n",
        "\n",
        "The model temp_c_boosted.pkl was successfully loaded from GitHub and applied to an inference dataset of 566,828 rows. Historical averages were merged for context, and actual temperatures were included for evaluation.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "\n",
        "*Mean Absolute Error* (MAE): 5.178\n",
        "\n",
        "*Root Mean Squared Error* (RMSE): 7.042\n",
        "\n",
        "*Robust MAE* (excluding top 1% of errors): 4.949\n",
        "\n",
        "*Robust RMSE* (excluding top 1% of errors): 6.480\n",
        "\n",
        "These metrics indicate strong predictive performance across the majority of the dataset, with robust metrics providing a clearer picture by reducing the influence of extreme outliers.\n",
        "\n",
        "Sample predictions demonstrate the model’s ability to capture general temperature trends across diverse locations: WASHINGTON D.C., BEIJING, MOSCOW, NEW DELHI, and LONDON, with predicted values closely aligning with actual measurements.\n",
        "\n",
        "All predictions have been saved to **test_predictions_clean.csv** for further analysis or reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_BDkzMzFpGUy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(df_results['abs_error'], bins=100, log=True)\n",
        "plt.xlabel('Absolute Error (°C)')\n",
        "plt.ylabel('Count (log scale)')\n",
        "plt.title('Error Distribution (Sep–Dec 2024)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QPmwz6-ZOsg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from datetime import datetime\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# ---------------- Setup ----------------\n",
        "MODEL_URL = \"https://github.com/praxavv/sustainability-tracker/raw/main/temp_c_boosted.pkl\"\n",
        "MODEL_PATH = \"temp_c_boosted.pkl\"\n",
        "\n",
        "# Download model fresh\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    os.remove(MODEL_PATH)\n",
        "urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "expected = list(getattr(model, \"feature_names_in_\", []))\n",
        "geolocator = Nominatim(user_agent=\"geoapi\")\n",
        "bq = bigquery.Client()\n",
        "\n",
        "# ---------------- Prediction Function ----------------\n",
        "def run_prediction(city, country, date):\n",
        "    try:\n",
        "        dt = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        month, day = dt.month, dt.day\n",
        "        output_html = \"<h3>Weather Prediction Dashboard 🌡️</h3>\"\n",
        "\n",
        "        # Step 1: Geocode\n",
        "        loc = geolocator.geocode(f\"{city}, {country}\")\n",
        "        if not loc:\n",
        "            display(HTML(f\"<p style='color:red;'>⚠️ Could not geocode {city}, {country}</p>\"))\n",
        "            return\n",
        "        lat, lon = round(loc.latitude, 2), round(loc.longitude, 2)\n",
        "        output_html += f\"<p>✅ Coordinates for <b>{city.strip()}, {country.strip()}</b> → lat={lat}, lon={lon}</p>\"\n",
        "\n",
        "        # Step 2: Nearest training row\n",
        "        query = \"\"\"\n",
        "        SELECT *\n",
        "        FROM `sustainability-tracker-469906.weather_models.gsod_train_features`\n",
        "        WHERE month=@month AND day=@day\n",
        "        ORDER BY SQRT(POWER(lat-@lat,2)+POWER(lon-@lon,2)) ASC\n",
        "        LIMIT 1\n",
        "        \"\"\"\n",
        "        job_config = bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ScalarQueryParameter(\"month\", \"INT64\", month),\n",
        "                bigquery.ScalarQueryParameter(\"day\", \"INT64\", day),\n",
        "                bigquery.ScalarQueryParameter(\"lat\", \"FLOAT64\", lat),\n",
        "                bigquery.ScalarQueryParameter(\"lon\", \"FLOAT64\", lon),\n",
        "            ]\n",
        "        )\n",
        "        nearest_df = bq.query(query, job_config=job_config).to_dataframe()\n",
        "        if nearest_df.empty:\n",
        "            display(HTML(\"<p style='color:red;'>⚠️ No training row found near this location.</p>\"))\n",
        "            return\n",
        "        nearest = nearest_df.iloc[0].round(2).to_dict()\n",
        "        output_html += f\"<p>✅ Features filled from nearest training row at lat={nearest['lat']}, lon={nearest['lon']}</p>\"\n",
        "\n",
        "        # Step 3: Build feature row\n",
        "        row = {}\n",
        "        for f in expected:\n",
        "            if f in nearest:\n",
        "                row[f] = nearest[f]\n",
        "            elif f == \"lat\":\n",
        "                row[f] = lat\n",
        "            elif f == \"lon\":\n",
        "                row[f] = lon\n",
        "            elif f == \"year\":\n",
        "                row[f] = dt.year\n",
        "            elif f == \"month\":\n",
        "                row[f] = month\n",
        "            elif f == \"day\":\n",
        "                row[f] = day\n",
        "            else:\n",
        "                row[f] = np.nan\n",
        "\n",
        "        # Step 4: Prediction\n",
        "        X = pd.DataFrame([row], columns=expected)\n",
        "        pred = round(float(model.predict(X)[0]), 2)\n",
        "        output_html += f\"<p>✅ Model Prediction = <b>{pred:.2f}°C</b></p>\"\n",
        "\n",
        "        # Step 5: Nearest station\n",
        "        station_query = \"\"\"\n",
        "        SELECT station_name, latitude AS lat, longitude AS lon\n",
        "        FROM `sustainability-tracker-469906.weather_models.gsod_daily_aggregated`\n",
        "        ORDER BY SQRT(POWER(latitude-@lat,2)+POWER(longitude-@lon,2)) ASC\n",
        "        LIMIT 1\n",
        "        \"\"\"\n",
        "        station_df = bq.query(station_query, job_config=bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ScalarQueryParameter(\"lat\", \"FLOAT64\", lat),\n",
        "                bigquery.ScalarQueryParameter(\"lon\", \"FLOAT64\", lon),\n",
        "            ]\n",
        "        )).to_dataframe()\n",
        "        station_name = station_df.station_name.iloc[0] if not station_df.empty else \"unknown\"\n",
        "        output_html += f\"<p>✅ Nearest Station → <b>{station_name}</b></p>\"\n",
        "        output_html += f\"<h2>🌡️ Predicted Temp on {date}: {pred:.2f}°C</h2>\"\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(output_html))\n",
        "\n",
        "    except Exception as e:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(f\"<p style='color:red;'>⚠️ Error: {e}</p>\"))\n",
        "\n",
        "\n",
        "# ---------------- Input Widgets ----------------\n",
        "FIELD_WIDTH = \"300px\"\n",
        "DESC_WIDTH = \"110px\"\n",
        "\n",
        "city_input = widgets.Text(\n",
        "    description=\"City:\",\n",
        "    placeholder=\"e.g., New York\",\n",
        "    layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "    style={\"description_width\": DESC_WIDTH},\n",
        ")\n",
        "\n",
        "country_input = widgets.Text(\n",
        "    description=\"Country Code:\",\n",
        "    placeholder=\"(e.g. US, IN)\",\n",
        "    layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "    style={\"description_width\": DESC_WIDTH},\n",
        ")\n",
        "\n",
        "date_input = widgets.DatePicker(\n",
        "    description=\"Date:\",\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width=FIELD_WIDTH),\n",
        "    style={\"description_width\": DESC_WIDTH},\n",
        ")\n",
        "\n",
        "predict_button = widgets.Button(\n",
        "    description=\"Predict 🌡️\",\n",
        "    button_style=\"success\",\n",
        "    layout=widgets.Layout(width=\"160px\")\n",
        ")\n",
        "\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def on_predict_clicked(b):\n",
        "    with output_widget:\n",
        "        if not date_input.value:\n",
        "            display(HTML(\"<p style='color:red;'>⚠️ Please select a date.</p>\"))\n",
        "            return\n",
        "        run_prediction(city_input.value.strip(), country_input.value.strip(), date_input.value.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "predict_button.on_click(on_predict_clicked)\n",
        "\n",
        "# ---------------- Layout ----------------\n",
        "inputs = widgets.VBox([city_input, country_input, date_input,\n",
        "                       widgets.HBox([widgets.Box(layout=widgets.Layout(flex=\"1\")), predict_button],\n",
        "                                    layout=widgets.Layout(width=\"304.5px\"))])\n",
        "\n",
        "dashboard = widgets.VBox([inputs, output_widget],\n",
        "                         layout=widgets.Layout(align_items=\"flex-start\"))\n",
        "print(\"Example Input: New York, US\\n\")\n",
        "display(dashboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh3n18CGqbvE"
      },
      "source": [
        "> **Impact Statement**\n",
        "\n",
        "By converting cleaned NOAA GSOD data (2018–2024) into an interactive globe with integrated predictive models, ClimateCast empowers diverse stakeholders—farmers, energy planners, and city officials—to visually explore forecasts, run scenario experiments, and make more informed operational decisions. When paired with the experiment widget, the platform facilitates rapid hypothesis testing and localised decision support, lowering the barrier between climate data and action.\n",
        "\n",
        ">**Conclusion**\n",
        "\n",
        "This work demonstrates a practical integration of historical data, machine learning, and interactive visualization to deliver interpretable, actionable short-term temperature forecasts. The dashboard is production-ready in design: it uses BigQuery for scalable data access, supports batch inference and interactive exploration, and includes a public notebook and reproducible codebase for verification. While results are promising, coverage and local accuracy vary with station density and data quality—future work will focus on expanding spatial coverage, quantifying uncertainty, and reducing inference latency."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
